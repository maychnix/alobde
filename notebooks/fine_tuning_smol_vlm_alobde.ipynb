{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKadZFQ2IdJb"
   },
   "source": [
    "# Fine-tuning SmolVLM with TRL on a consumer GPU\n",
    "\n",
    "Notebook based on: https://huggingface.co/learn/cookbook/fine_tuning_smol_vlm_sft_trl\n",
    "\n",
    "Ressources provided by Scientific Computing University of Leipzig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSHmDKNFoqjC"
   },
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Let‚Äôs start by installing the essential libraries we‚Äôll need for fine-tuning! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GCMhPmFdIGSb"
   },
   "outputs": [],
   "source": [
    "!pip install  -U -q transformers trl datasets bitsandbytes peft accelerate\n",
    "# Tested with transformers==4.46.3, trl==0.12.1, datasets==3.1.0, bitsandbytes==0.45.0, peft==0.13.2, accelerate==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x6fAqSnKDtKg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.4.post1-cp312-cp312-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.12/site-packages (from flash-attn) (2.4.1+cu121)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /software/jupyterlab/conda/lib/python3.12/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: networkx in /software/jupyterlab/conda/lib/python3.12/site-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /software/jupyterlab/conda/lib/python3.12/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /software/jupyterlab/conda/lib/python3.12/site-packages (from torch->flash-attn) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /software/jupyterlab/conda/lib/python3.12/site-packages (from torch->flash-attn) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.12/site-packages (from torch->flash-attn) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/jupyterlab/conda/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /software/jupyterlab/conda/lib/python3.12/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0-2Lso6wkIh"
   },
   "source": [
    "Authenticate with your Hugging Face account to save and share your model directly from this notebook üóùÔ∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xcL4-bwGIoaR"
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9QXwbJ7ovM5"
   },
   "source": [
    "## 2. Load Dataset üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBWqnXkcTN-s"
   },
   "source": [
    "We‚Äôll create a system message to make the VLM act as a chart analysis expert, giving concise answers about historic alchemic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IBvKAlXhI46X"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting alchemic objects in historic book illustrations.\n",
    "Your task is to analyze the provided book page and respond in JSON format only. In total there are 12 classes of alchemic objects you should detect.\n",
    "The classes are: ampullae, animal, cucurbitae, cucurbitae-ambix, ollae, cucurbitae-retorte, cucurbitae-rosenhut, furnace, human, mineral-metal, other-equipment, plant.\n",
    "Respond in a well-structured JSON format in which you always name all of the 12 classes and then their number of occurences in the provided image.\n",
    "An output should look like this, for example: \n",
    "{\n",
    "  \"ampullae\": 0,\n",
    "  \"animal\": 0,\n",
    "  \"cucurbitae\": 0,\n",
    "  \"cucurbitae-ambix\": 0,\n",
    "  \"cucurbitae-retorte\": 0,\n",
    "  \"cucurbitae-rosenhut\": 0,\n",
    "  \"furnace\": 0,\n",
    "  \"human\": 0,\n",
    "  \"mineral-metal\": 0,\n",
    "  \"other-equipment\": 0,\n",
    "  \"plant\": 0,\n",
    "  \"ollae\": 0\n",
    "}\n",
    "Focus on delivering accurate, succinct answers based on the visual information. \n",
    "Don't add any additional explanation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Which alchemic objects are in the image?\n",
    "Provide an answer in a well-structured JSON format with 12 classes (ampullae, animal, cucurbitae, cucurbitae-ambix,\n",
    "ollae, cucurbitae-retorte, cucurbitae-rosenhut, furnace, human, mineral-metal,\n",
    "other-equipment, plant) and the number of their occurences.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8UpL5ZxTakm"
   },
   "source": [
    "We‚Äôll format the dataset into a chatbot structure, with the system message, image, user query, and answer for each interaction.\n",
    "\n",
    "üí°For more tips on using this model, check out the [Model Card](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XG8NuzqjjbgI"
   },
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_message\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": query,\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": sample[\"labels\"]\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QFe_A78aIwK8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"maychnix/AlchObj\"\n",
    "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train', 'valid', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0X4ZajcPV10"
   },
   "source": [
    "Let‚Äôs take a look at the dataset structure. It includes an image, a query, a label (the answer), and a fourth feature that we‚Äôll be discarding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2UKZj15jGwv",
    "outputId": "773e9dc2-4477-47e1-8992-7e777c786a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'labels', 'idx', 'source'],\n",
       "    num_rows: 434\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1edUqNGWTtjA"
   },
   "source": [
    "Now, let‚Äôs format the data using the chatbot structure. This will set up the interactions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oSHNqk0dkxii"
   },
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlp3WnfEuftT",
    "outputId": "7bf3b2df-005a-4960-b7f8-07b7c24b560e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'You are a Vision Language Model specialized in interpreting alchemic objects in historic book illustrations.\\nYour task is to analyze the provided book page and respond in JSON format only. In total there are 12 classes of alchemic objects you should detect.\\nThe classes are: ampullae, animal, cucurbitae, cucurbitae-ambix, ollae, cucurbitae-retorte, cucurbitae-rosenhut, furnace, human, mineral-metal, other-equipment, plant.\\nRespond in a well-structured JSON format in which you always name all of the 12 classes and then their number of occurences in the provided image.\\nAn output should look like this, for example: \\n{\\n  \"ampullae\": 0,\\n  \"animal\": 0,\\n  \"cucurbitae\": 0,\\n  \"cucurbitae-ambix\": 0,\\n  \"cucurbitae-retorte\": 0,\\n  \"cucurbitae-rosenhut\": 0,\\n  \"furnace\": 0,\\n  \"human\": 0,\\n  \"mineral-metal\": 0,\\n  \"other-equipment\": 0,\\n  \"plant\": 0,\\n  \"ollae\": 0\\n}\\nFocus on delivering accurate, succinct answers based on the visual information. \\nDon\\'t add any additional explanation.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1346>},\n",
       "   {'type': 'text',\n",
       "    'text': 'Which alchemic objects are in the image?\\nProvide an answer in a well-structured JSON format with 12 classes (ampullae, animal, cucurbitae, cucurbitae-ambix,\\nollae, cucurbitae-retorte, cucurbitae-rosenhut, furnace, human, mineral-metal,\\nother-equipment, plant) and the number of their occurences.'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': {'ampullae': 0,\n",
       "     'animal': 0,\n",
       "     'cucurbitae': 1,\n",
       "     'cucurbitae-ambix': 1,\n",
       "     'cucurbitae-retorte': 0,\n",
       "     'cucurbitae-rosenhut': 1,\n",
       "     'furnace': 1,\n",
       "     'human': 0,\n",
       "     'mineral-metal': 0,\n",
       "     'ollae': 0,\n",
       "     'other-equipment': 0,\n",
       "     'plant': 0}}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY1Y_KDtoycB"
   },
   "source": [
    "## 3. Load Model and Check Performance! ü§î\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PCJhM6tCw4lq"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.idefics3.modeling_idefics3 because of the following error (look up to see its traceback):\n/home/sc.uni-leipzig.de/cm77xisu/.local/lib/python3.12/site-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZNK3c1011StorageImpl27throw_data_ptr_access_errorEv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1883\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/software/jupyterlab/conda/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/idefics3/modeling_idefics3.py:30\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutput, ModelOutput\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     add_start_docstrings,\n\u001b[1;32m     33\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:54\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _load_state_dict_into_zero3_model\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attention_forward\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flex_attention_forward\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/integrations/flash_attention.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flash_attention_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _flash_attention_forward\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flash_attn_greater_or_equal_2_10\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py:30\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flash_attn_2_available():\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert_padding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_first_axis, pad_input, unpad_input  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attn_func, flash_attn_varlen_func\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/flash_attn/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.7.4.post1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     flash_attn_func,\n\u001b[1;32m      5\u001b[0m     flash_attn_kvpacked_func,\n\u001b[1;32m      6\u001b[0m     flash_attn_qkvpacked_func,\n\u001b[1;32m      7\u001b[0m     flash_attn_varlen_func,\n\u001b[1;32m      8\u001b[0m     flash_attn_varlen_kvpacked_func,\n\u001b[1;32m      9\u001b[0m     flash_attn_varlen_qkvpacked_func,\n\u001b[1;32m     10\u001b[0m     flash_attn_with_kvcache,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/flash_attn/flash_attn_interface.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflash_attn_2_cuda\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mflash_attn_gpu\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/sc.uni-leipzig.de/cm77xisu/.local/lib/python3.12/site-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZNK3c1011StorageImpl27throw_data_ptr_access_errorEv",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Idefics3ForConditionalGeneration, AutoProcessor\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceTB/SmolVLM-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1872\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1871\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1872\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1874\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1871\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1869\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1871\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1872\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1885\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1886\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1887\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1888\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.idefics3.modeling_idefics3 because of the following error (look up to see its traceback):\n/home/sc.uni-leipzig.de/cm77xisu/.local/lib/python3.12/site-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZNK3c1011StorageImpl27throw_data_ptr_access_errorEv"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HobU2iPUDWL"
   },
   "source": [
    "Next, we‚Äôll load the model and the tokenizer to prepare for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awtjIq86JfFF"
   },
   "outputs": [],
   "source": [
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpLfsCUtUW6I"
   },
   "source": [
    "Let‚Äôs create a method that takes the model, processor, and sample as inputs to generate the model's answer. This will allow us to streamline the inference process and easily evaluate the VLM's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MoRTjFcE8qD"
   },
   "outputs": [],
   "source": [
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=256, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[1:2],  # Use the sample without the system message\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs = []\n",
    "    image = sample[1]['content'][0]['image']\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    image_inputs.append([image])\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        #text=[text_input],\n",
    "        text=text_input,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5UeNiMJC_uCk",
    "outputId": "d0d39070-4221-4c38-928d-75ebac8a3479"
   },
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, processor, train_dataset[42])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysh0e9DRUfF-"
   },
   "source": [
    "It seems like the model is not able to output clean JSON nor accuratly understand the task, we can fine-tune the model with more relevant data to ensure it better understands the context and provides more accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3b76rawti6"
   },
   "source": [
    "**Remove Model and Clean GPU**\n",
    "\n",
    "Before we proceed with training the model in the next section, let's clear the current variables and clean the GPU to free up resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxkXZuUkvy8j",
    "outputId": "d75c1c7b-04d0-468c-8d45-9a7b3e7f5426"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'processor' in globals(): del globals()['processor']\n",
    "    if 'trainer' in globals(): del globals()['trainer']\n",
    "    if 'peft_model' in globals(): del globals()['peft_model']\n",
    "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIZOIVEzQqNg"
   },
   "source": [
    "## 4. Fine-Tune the Model using TRL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrR9gP2z90z"
   },
   "source": [
    "### 4.1 Load the Quantized Model for Training ‚öôÔ∏è\n",
    "\n",
    "Next, we‚Äôll load the quantized model using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index). If you want to learn more about quantization, check out [this blog post](https://huggingface.co/blog/merve/quantization) or [this one](https://www.maartengrootendorst.com/blog/quantization/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm_bJRrXsESg"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    _attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65wfO29isQlX"
   },
   "source": [
    "### 4.2 Set Up QLoRA and SFTConfig üöÄ\n",
    "\n",
    "Next, we‚Äôll configure [QLoRA](https://github.com/artidoro/qlora) for our training setup. QLoRA allows efficient fine-tuning of large models by reducing the memory footprint. Unlike traditional LoRA, which uses low-rank approximation, QLoRA further quantizes the LoRA adapter weights, leading to even lower memory usage and faster training.\n",
    "\n",
    "To boost efficiency, we can also leverage a **paged optimizer** or **8-bit optimizer** during QLoRA implementation. This approach enhances memory efficiency and speeds up computations, making it ideal for optimizing our model without sacrificing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITmkRHWCKYjf",
    "outputId": "cd621500-ce1f-470d-d1a8-21ab026c36cb"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbqX1pQUKaSM"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"smolvlm-instruct-trl-sft-AlchObj\",\n",
    "    overwrite_output_dir=True, # added (ran notebook multiple times)\n",
    "    torch_empty_cache_steps=25, # added (oom error after ~70 steps)\n",
    "    num_train_epochs=12,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOUrD9P-y-Kf"
   },
   "source": [
    "### 4.3 Training the Model üèÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucTUbGURV2_-"
   },
   "source": [
    "To ensure that the data is correctly structured for the model during training, we need to define a collator function. This function will handle the formatting and batching of our dataset inputs, ensuring the data is properly aligned for training.\n",
    "\n",
    "üëâ For more details, check out the official [TRL example scripts](https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm_smol_vlm.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAzDovzylQeZ"
   },
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "\n",
    "    image_inputs = []\n",
    "    for example in examples:\n",
    "      image = example[1]['content'][0]['image']\n",
    "      if image.mode != 'RGB':\n",
    "          image = image.convert('RGB')\n",
    "      image_inputs.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "    labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skbpTuJlV8qN"
   },
   "source": [
    "Now, we will define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer), which is a wrapper around the [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the [PeftModel](https://huggingface.co/docs/peft/v0.6.0/package_reference/peft_model) when a [PeftConfig](https://huggingface.co/docs/peft/v0.6.0/en/package_reference/config#peft.PeftConfig) object is provided. By using `SFTTrainer`, we can efficiently manage the training workflow and ensure a smooth fine-tuning experience for our Vision Language Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_jk-U7ULYtA"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlDsh4WvWCx0"
   },
   "source": [
    "Time to Train the Model! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "p1rgMTBDLboO",
    "outputId": "9da86b78-a432-459f-94c3-89025d6059ce"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6CykSCtX-Xa"
   },
   "source": [
    "Let's save the results üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE8usZw0lgrL"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yx_sGW42dN3"
   },
   "source": [
    "## 5. Testing the Fine-Tuned Model üîç\n",
    "\n",
    "Now that our Vision Language Model (VLM) is fine-tuned, it's time to evaluate its performance! In this section, we'll test the model using examples from the ChartQA dataset to assess how accurately it answers questions based on chart images. Let's dive into the results and see how well it performs! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0KEPu6qYKqn"
   },
   "source": [
    "Let's clean up the GPU memory to ensure optimal performance üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ttx6EK8Uy8t0",
    "outputId": "e671dad3-bddc-4206-dcb9-78a55cc242c0"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCTPHsfujn2"
   },
   "source": [
    "We will reload the base model using the same pipeline as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFqTNUud2lA7"
   },
   "outputs": [],
   "source": [
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRAPEYKkYSkB"
   },
   "source": [
    "We will attach the trained adapter to the pretrained model. This adapter contains the fine-tuning adjustments made during training, enabling the base model to leverage the new knowledge while keeping its core parameters intact. By integrating the adapter, we enhance the model's capabilities without altering its original structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQi2xBXk4sHe"
   },
   "outputs": [],
   "source": [
    "adapter_path = \"maychnix/smolvlm-instruct-trl-sft-AlchObj\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swibyq5AWctZ"
   },
   "source": [
    "Let's evaluate the model on an unseen sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czZSBgnoef1E",
    "outputId": "515a198d-55d9-4494-8fb7-7193c899b3d3"
   },
   "outputs": [],
   "source": [
    "test_dataset[20][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "ATuQ6ZS6eirO",
    "outputId": "076831d3-52c3-4506-9a09-47b46a2d5d01"
   },
   "outputs": [],
   "source": [
    "test_dataset[20][1]['content'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9yHJMKHNWcMc",
    "outputId": "fef11c29-f8ed-4301-9f16-e4b963587988"
   },
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, processor, test_dataset[20])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUr6jmnAIlh1"
   },
   "source": [
    "The model has successfully learned to respond to the queries as specified in the dataset. We've achieved our goal! üéâ‚ú®"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
